{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7: CNNs & RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your name and UFL email address\n",
    "name = 'enter your name'\n",
    "email = 'enter your email'\n",
    "\n",
    "name = 'solution' # ###- \n",
    "email = 'solution' # ###- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignment 7 -- name: solution, email: solution\n",
      "\n",
      "### Python version: 3.8.5 (default, Jan 27 2021, 15:41:15) \n",
      "[GCC 9.3.0]\n",
      "### NumPy version: 1.19.5\n",
      "### Scikit-learn version: 0.24.0\n",
      "### Tensorflow version: 2.4.0\n",
      "### TF Keras version: 2.4.0\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "if name == 'enter your name' or email == 'enter your email':\n",
    "    assert False, 'Enter your name & email first!'\n",
    "else:\n",
    "    print('Assignment 7 -- name: {}, email: {}\\n'.format(name, email))\n",
    "    \n",
    "    # Load packages we need\n",
    "    import sys\n",
    "    import os\n",
    "    import time\n",
    "\n",
    "    import numpy as np\n",
    "    import sklearn\n",
    "    \n",
    "    # we'll use tensorflow and keras for neural networks\n",
    "    import tensorflow as tf\n",
    "    import tensorflow.keras as keras\n",
    "    \n",
    "    # import layers we may use\n",
    "    from tensorflow.keras.layers import Input, Flatten, Dense, Conv2D, MaxPooling2D, Dropout\n",
    "\n",
    "    # import callbacks we may use\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "    \n",
    "    # Load the TensorBoard notebook extension\n",
    "    #%load_ext tensorboard\n",
    "\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "    # Let's check our software versions\n",
    "    print('### Python version: ' + __import__('sys').version)\n",
    "    print('### NumPy version: ' + np.__version__)\n",
    "    print('### Scikit-learn version: ' + sklearn.__version__)\n",
    "    print('### Tensorflow version: ' + tf.__version__)\n",
    "    print('### TF Keras version: ' + keras.__version__)\n",
    "    print('------------')\n",
    "\n",
    "\n",
    "    # load our packages / code\n",
    "    sys.path.insert(1, '../common/')\n",
    "    import utils\n",
    "    import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters to control behavior of the pre-processing, ML, analysis, etc.\n",
    "seed = 42\n",
    "\n",
    "# deterministic seed for reproducibility\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "prop_vec = [24, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 1] (20 points) Loading and Processing CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 1a] (20 points) Complete the implementation of load_preprocess_cifar10(). Make sure you correctly implement all of the cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# refer to: https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10/load_data\n",
    "# and to https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "def load_preprocess_cifar10(onehot=True, minmax_normalize=True):\n",
    "    \n",
    "    ###* put your code here (~10-20 lines) *###\n",
    "    labels = np.array(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "    train, testval = cifar10.load_data()\n",
    "    \n",
    "    train_x, train_y = train\n",
    "    testval_x, testval_y = testval\n",
    "    \n",
    "    if minmax_normalize:\n",
    "        train_x = train_x / 255.0\n",
    "        testval_x = testval_x / 255.0\n",
    "        \n",
    "    if onehot:\n",
    "        train_y = keras.utils.to_categorical(train_y, labels.shape[0])\n",
    "        testval_y = keras.utils.to_categorical(testval_y, labels.shape[0])\n",
    "    \n",
    "    # split test - val\n",
    "    nval = testval_x.shape[0] // 2\n",
    "    \n",
    "    val_x = testval_x[:nval]\n",
    "    val_y = testval_y[:nval]\n",
    "    \n",
    "    test_x = testval_x[nval:]\n",
    "    test_y = testval_y[nval:]\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, val_x, val_y, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some sanity checks\n",
    "train_x, train_y, test_x, test_y, val_x, val_y, labels = load_preprocess_cifar10(onehot=False, minmax_normalize=False)\n",
    "assert train_x.shape[0] == train_y.shape[0] and test_x.shape[0] == test_y.shape[0] and val_x.shape[0] == val_y.shape[0]\n",
    "assert np.amax(train_x) >= 255 and np.amax(test_x) >= 255 and np.amax(val_x) >= 255\n",
    "assert train_y.shape == (train_y.shape[0],) or train_y.shape == (train_y.shape[0],1)\n",
    "\n",
    "train_x, train_y, test_x, test_y, val_x, val_y, labels = load_preprocess_cifar10(onehot=True, minmax_normalize=False)\n",
    "assert np.amax(train_x) >= 255 and np.amax(test_x) >= 255 and np.amax(val_x) >= 255\n",
    "assert train_y.shape == (train_y.shape[0],10) and train_y.shape[1] == test_y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually load the data\n",
    "train_x, train_y, test_x, test_y, val_x, val_y, labels = load_preprocess_cifar10()\n",
    "assert np.amax(train_x) <= 1 and np.amax(test_x) <= 1 and np.amax(val_x) <= 1\n",
    "assert np.amax(train_x) >= 0 and np.amax(test_x) >= 0 and np.amax(val_x) >= 0\n",
    "\n",
    "assert labels.shape[0] == 10 and labels.shape[0] == train_y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (5000, 32, 32, 3) (5000, 32, 32, 3) (50000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape, val_x.shape, test_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 2] (25 points) Training a CNN for Cifar-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use the following architecture\n",
    "- Conv layer with 32 filters, (3,3) filter size, stride of 1, padding 'same'\n",
    "- Conv layer with 32 filters, (3,3) filter size, stride of 1, padding 'same'\n",
    "- Max pooling layer (2,2)\n",
    "- Dropout with rate 25%\n",
    "- Conv layer with 64 filters, (3,3) filter size, stride of 1, padding 'same'\n",
    "- Conv layer with 64 filters, (3,3) filter size, stride of 1, padding 'same'\n",
    "- Max pooling layer (2,2)\n",
    "- Dropout with rate 25%\n",
    "- Conv layer with 128 filters, (3,3) filter size, stride of 1, padding 'same'\n",
    "- Conv layer with 128 filters, (3,3) filter size, stride of 1, padding 'same'\n",
    "- Max pooling layer (2,2)\n",
    "- Dropout with rate 25%\n",
    "- Flatten\n",
    "- FC with 128 units\n",
    "- Dropout with rate 25%\n",
    "- FC with 64 units\n",
    "- Dropout with rate 25%\n",
    "- (Output layer) FC with 10 units\n",
    "\n",
    "#### For all layers (if applicable) except the output layer you should use:\n",
    "- ReLU as activation function\n",
    "- He uniform weight initialization strategy\n",
    "- L2 regularization with regularization constant set to 0.001\n",
    "\n",
    "#### For the output layer you should select a suitable activation function that is consistent with the task and loss function you use. Use Adam for the optimizer with learning rate 0.002."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2a] (15 points) Implement create_compile_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compile_cnn(input_shape=[32, 32, 3], num_outputs=10, verbose=False):\n",
    "    \n",
    "    model = keras.models.Sequential(name='CIFAR-10--CNN')\n",
    "    \n",
    "    ### Don't forget to compile the model and print the summary if verbose=True\n",
    "    ###* put your code here (~20 lines) *###\n",
    "    \n",
    "    l2reg = keras.regularizers.l2(0.001)\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2reg, padding='same', \n",
    "                     input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2reg, padding='same', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25, name='dropout-conv1'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2reg, padding='same'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2reg, padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25, name='dropout-conv2'))\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2reg, padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2reg, padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25, name='dropout-conv3'))\n",
    "    \n",
    "    model.add(Flatten(name='flatten'))\n",
    "    \n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2reg, name='fc1'))\n",
    "    model.add(Dropout(0.25, name='dropout-fc1'))\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2reg, name='fc2'))\n",
    "    model.add(Dropout(0.25, name='dropout-fc2'))\n",
    "    \n",
    "    model.add(Dense(num_outputs, activation='softmax', name='output'))\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=0.002)\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CIFAR-10--CNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout-conv1 (Dropout)      (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout-conv2 (Dropout)      (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout-conv3 (Dropout)      (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "dropout-fc1 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout-fc2 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 558,186\n",
      "Trainable params: 558,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "_ = create_compile_cnn(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2b] (10 points) Train the model. Fill in the implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "782/782 [==============================] - 157s 199ms/step - loss: 2.7462 - accuracy: 0.2010 - val_loss: 1.7624 - val_accuracy: 0.4140\n",
      "Epoch 2/15\n",
      "782/782 [==============================] - 172s 220ms/step - loss: 1.7836 - accuracy: 0.3962 - val_loss: 1.5142 - val_accuracy: 0.5056\n",
      "Epoch 3/15\n",
      "782/782 [==============================] - 170s 217ms/step - loss: 1.6218 - accuracy: 0.4687 - val_loss: 1.4836 - val_accuracy: 0.5324\n",
      "Epoch 4/15\n",
      "782/782 [==============================] - 169s 216ms/step - loss: 1.5293 - accuracy: 0.5078 - val_loss: 1.3760 - val_accuracy: 0.5802\n",
      "Epoch 5/15\n",
      "782/782 [==============================] - 173s 221ms/step - loss: 1.4676 - accuracy: 0.5455 - val_loss: 1.3058 - val_accuracy: 0.5972\n",
      "Epoch 6/15\n",
      "782/782 [==============================] - 172s 220ms/step - loss: 1.4297 - accuracy: 0.5590 - val_loss: 1.2490 - val_accuracy: 0.6176\n",
      "Epoch 7/15\n",
      "782/782 [==============================] - 173s 222ms/step - loss: 1.3882 - accuracy: 0.5799 - val_loss: 1.2773 - val_accuracy: 0.6288\n",
      "Epoch 8/15\n",
      "782/782 [==============================] - 173s 221ms/step - loss: 1.3686 - accuracy: 0.5949 - val_loss: 1.2206 - val_accuracy: 0.6476\n",
      "Epoch 9/15\n",
      "782/782 [==============================] - 176s 226ms/step - loss: 1.3465 - accuracy: 0.6016 - val_loss: 1.1741 - val_accuracy: 0.6666\n",
      "Epoch 10/15\n",
      "782/782 [==============================] - 180s 230ms/step - loss: 1.3217 - accuracy: 0.6167 - val_loss: 1.1319 - val_accuracy: 0.6770\n",
      "Epoch 11/15\n",
      "782/782 [==============================] - 181s 231ms/step - loss: 1.3128 - accuracy: 0.6192 - val_loss: 1.1513 - val_accuracy: 0.6638\n",
      "Epoch 12/15\n",
      "782/782 [==============================] - 182s 233ms/step - loss: 1.3012 - accuracy: 0.6271 - val_loss: 1.1169 - val_accuracy: 0.6938\n",
      "Epoch 13/15\n",
      "782/782 [==============================] - 182s 232ms/step - loss: 1.2965 - accuracy: 0.6272 - val_loss: 1.1350 - val_accuracy: 0.6834\n",
      "Epoch 14/15\n",
      "782/782 [==============================] - 180s 231ms/step - loss: 1.2742 - accuracy: 0.6331 - val_loss: 1.1597 - val_accuracy: 0.6680\n",
      "Epoch 15/15\n",
      "782/782 [==============================] - 183s 234ms/step - loss: 1.2842 - accuracy: 0.6332 - val_loss: 1.1103 - val_accuracy: 0.6850\n"
     ]
    }
   ],
   "source": [
    "cnn_model_fp = './cifar10-cnn.h5'\n",
    "\n",
    "# If the model file exists, load it. Otherwise train it and save the model.\n",
    "# Note: if you need to retrain the model, simply delete the h5 file.\n",
    "if os.path.exists(cnn_model_fp):\n",
    "    model = keras.models.load_model(cnn_model_fp)\n",
    "else:\n",
    "    model = create_compile_cnn(verbose=False)\n",
    "    \n",
    "    # do the training using model.fit() for at least 3 epochs and your chosen batch_size\n",
    "    # you can set any callback you want on it, including checkpoint, early stopping, etc.\n",
    "    ###* put your code here (~3-5 lines) *###\n",
    "    early_stop_cb = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    max_epochs = 15\n",
    "    batch_size = 64\n",
    "\n",
    "    hist = model.fit(train_x, train_y, epochs=max_epochs, batch_size=batch_size, validation_data=(val_x, val_y), callbacks=[early_stop_cb])\n",
    "    \n",
    "    # save the model\n",
    "    model.save(cnn_model_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's evaluate the model on the test data\n",
    "loss, acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "print('[Model] Test accuracy: {:.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 3] (15 points) Processing Sequence Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3a] (15 points) Fill in the implementation of load_preprocess_imdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# the size of the vocabulary we'll use\n",
    "vocab_size = 12000\n",
    "maxlen = 150\n",
    "\n",
    "# refer to: https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb\n",
    "def load_preprocess_imdb(num_words=vocab_size, prop_vec=prop_vec, maxlen=maxlen, vectorize=False):\n",
    "    \n",
    "    np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
    "    \n",
    "    train, testval = imdb.load_data(num_words=num_words, maxlen=maxlen, oov_char=0)\n",
    "    \n",
    "    np.warnings.filterwarnings('default', category=np.VisibleDeprecationWarning)    \n",
    "    \n",
    "    ### Process the data \n",
    "    ### Merge train and testval, but then split again into train, test, val sets (according to prop_vec). You can use utils.train_test_val_split().)\n",
    "    ### - If vectorize=True, then you must encode the features of each example into vectors of vocab_size entries \n",
    "    ### such that entry i contains the number of time word i appeared in the sequence\n",
    "    ### - If vectorize=False, then you must encode the features of each examples as a sequence of size maxlen (represented as a np.array()). \n",
    "    ### Make sure to pad sequences with 0 as appropriate.\n",
    "    ###* put your code here (~10-15 lines) *###\n",
    "    \n",
    "    def seq_to_vec_count(x):\n",
    "        ret = np.zeros((x.shape[0], num_words))\n",
    "        for i, seq in enumerate(x):\n",
    "            for x in seq:\n",
    "                ret[i, x] += 1\n",
    "        return ret\n",
    "    \n",
    "    def seq_to_2darray(x):\n",
    "        ret = np.zeros((x.shape[0], maxlen))\n",
    "        for i, seq in enumerate(x):\n",
    "            ret[i,:] = np.pad(seq, (0, maxlen - len(seq)), 'constant')\n",
    "        return ret    \n",
    "    \n",
    "    \n",
    "    train_x, train_y = train\n",
    "    testval_x, testval_y = testval\n",
    "    \n",
    "    if vectorize:\n",
    "        train_x = seq_to_vec_count(train_x)\n",
    "        testval_x = seq_to_vec_count(testval_x)\n",
    "    else:\n",
    "        train_x = seq_to_2darray(train_x)\n",
    "        testval_x = seq_to_2darray(testval_x)\n",
    "    \n",
    "    # put things back together\n",
    "    all_x = np.r_[np.array(train_x), np.array(testval_x)]\n",
    "    all_y = np.r_[np.array(train_y), np.array(testval_y)]\n",
    "    \n",
    "    \n",
    "    # split the data into train, test, val\n",
    "    train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(all_x, all_y, prop_vec, shuffle=True, seed=seed)\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = load_preprocess_imdb(vectorize=False)\n",
    "assert train_x.shape == (16281, maxlen) and train_y.shape == (train_x.shape[0],)\n",
    "\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = load_preprocess_imdb(vectorize=True)\n",
    "assert train_x.shape == (16281, vocab_size) and train_y.shape == (train_x.shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 4] (35 points) RNN for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load the data\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = load_preprocess_imdb(vectorize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4a] (35 points) Complete the code below to define an RNN architecture for sentiment analysis. The goal is to predict the sentiment of IMDB reviews. You can use any architecture you want, but a good place to start would be to use an embedding layer followed by some recurrent layers (e.g., LSTM, GRU, etc.). Keep the number of parameters of the model below 2m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compile_rnn(input_shape=[None], vocab_size=vocab_size, embedding_size=128, num_outputs=1, verbose=False):\n",
    "    \n",
    "    model = keras.models.Sequential(name='imdb-RNN')\n",
    "    \n",
    "    ### Don't forget to compile the model and print the summary if verbose=True\n",
    "    ###* put your code here (~20 lines) *###\n",
    "\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(keras.layers.Embedding(vocab_size, embedding_size, name='embedding'))\n",
    "    \n",
    "    model.add(keras.layers.GRU(128, return_sequences=True, name='gru1'))\n",
    "    model.add(keras.layers.GRU(128, name='gru2'))\n",
    "    \n",
    "    model.add(Dense(num_outputs, activation='sigmoid', name='output'))\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=0.002)\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"imdb-RNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         1536000   \n",
      "_________________________________________________________________\n",
      "gru1 (GRU)                   (None, None, 128)         99072     \n",
      "_________________________________________________________________\n",
      "gru2 (GRU)                   (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,734,273\n",
      "Trainable params: 1,734,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_compile_rnn(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "509/509 [==============================] - 133s 255ms/step - loss: 0.6936 - accuracy: 0.5084 - val_loss: 0.5022 - val_accuracy: 0.7721\n",
      "Epoch 2/3\n",
      "509/509 [==============================] - 137s 269ms/step - loss: 0.3485 - accuracy: 0.8539 - val_loss: 0.2516 - val_accuracy: 0.9071\n",
      "Epoch 3/3\n",
      "509/509 [==============================] - 139s 274ms/step - loss: 0.1227 - accuracy: 0.9597 - val_loss: 0.2656 - val_accuracy: 0.8916\n"
     ]
    }
   ],
   "source": [
    "early_stop_cb = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# feel free to tweak the batch size, number of epochs and callbacks.\n",
    "max_epochs = 3\n",
    "batch_size = 32\n",
    "\n",
    "hist = model.fit(train_x, train_y, epochs=max_epochs, batch_size=batch_size, validation_data=(val_x, val_y), callbacks=[early_stop_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] Test accuracy: 91.30%\n"
     ]
    }
   ],
   "source": [
    "# let's evaluate the model on the test data\n",
    "loss, acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "print('[Model] Test accuracy: {:.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [CIS6930 Additional Task -- Task 5] (25 points): DNN for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the previous task, we use an RNN for sentiment analysis. In this task you will use a neural network without any recurrent layers for the same task as a comparison.\n",
    "\n",
    "### We'll use the data in vectorized form for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the data in vectorized form\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = load_preprocess_imdb(vectorize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5a] (20 points) Complete the code below to define an architecture of your choice *without* any recurrent layers. The goal is to get the best model with the fewest number of parameters. Keep the number of parameters of the model below 2m and ideally similar to the model of Task 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compile_dnn(input_shape=[vocab_size], num_outputs=1, verbose=False):\n",
    "    \n",
    "    model = keras.models.Sequential(name='imdb-DNN')\n",
    "    \n",
    "    ### Don't forget to compile the model and print the summary if verbose=True\n",
    "    ###* put your code here (~10 lines) *###\n",
    "\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(Dense(128, activation='relu', name='fc1'))\n",
    "    model.add(Dropout(0.5, name='dropout1'))\n",
    "    model.add(Dense(96, activation='relu', name='fc2'))\n",
    "    model.add(Dropout(0.3, name='dropout2'))\n",
    "    model.add(Dense(48, activation='relu', name='fc3'))\n",
    "    model.add(Dropout(0.2, name='dropout3'))\n",
    "    model.add(Dense(16, activation='relu', name='fc4'))\n",
    "    model.add(Dropout(0.2, name='dropout4'))\n",
    "    \n",
    "    model.add(Dense(num_outputs, activation='sigmoid', name='output'))\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=0.001)\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"imdb-DNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "fc1 (Dense)                  (None, 128)               1536128   \n",
      "_________________________________________________________________\n",
      "dropout1 (Dropout)           (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 96)                12384     \n",
      "_________________________________________________________________\n",
      "dropout2 (Dropout)           (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "fc3 (Dense)                  (None, 48)                4656      \n",
      "_________________________________________________________________\n",
      "dropout3 (Dropout)           (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "fc4 (Dense)                  (None, 16)                784       \n",
      "_________________________________________________________________\n",
      "dropout4 (Dropout)           (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,553,969\n",
      "Trainable params: 1,553,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_compile_dnn(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 0.5746 - accuracy: 0.6777 - val_loss: 0.2824 - val_accuracy: 0.8850\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 0.2312 - accuracy: 0.9183 - val_loss: 0.2706 - val_accuracy: 0.8886\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 0.1568 - accuracy: 0.9473 - val_loss: 0.3100 - val_accuracy: 0.8857\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 0.0999 - accuracy: 0.9687 - val_loss: 0.3537 - val_accuracy: 0.8842\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 0.0760 - accuracy: 0.9750 - val_loss: 0.3679 - val_accuracy: 0.8813\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 0.0644 - accuracy: 0.9803 - val_loss: 0.4491 - val_accuracy: 0.8805\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 0.0490 - accuracy: 0.9839 - val_loss: 0.5000 - val_accuracy: 0.8850\n"
     ]
    }
   ],
   "source": [
    "early_stop_cb = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# feel free to tweak the batch size, number of epochs and callbacks.\n",
    "max_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "hist = model.fit(train_x, train_y, epochs=max_epochs, batch_size=batch_size,validation_data=(val_x, val_y), callbacks=[early_stop_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] Test accuracy: 90.79%\n"
     ]
    }
   ],
   "source": [
    "# let's evaluate the model on the test data\n",
    "loss, acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "print('[Model] Test accuracy: {:.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5b] (5 points) Compare this model to the model of Task 4. What do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer here *###\n",
    "#\n",
    "# The RNN model is (slightly) better in terms of performance but takes much longer to train. \n",
    "# \n",
    "# A plausible explanation is that even though the data is sequential, the order of words do not matter \n",
    "# that much to whether a review is positive or negative, thus ignoring the sequential nature of the data \n",
    "# does not harm model performance significantly. \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
