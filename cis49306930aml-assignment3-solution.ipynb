{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Regression & Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your name and UFL email address\n",
    "name = 'enter your name'\n",
    "email = 'enter your email'\n",
    "\n",
    "name = 'solution' # ###- \n",
    "email = 'solution' # ###- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignment 3 -- name: solution, email: solution\n",
      "\n",
      "### Python version: 3.8.5 (default, Jan 27 2021, 15:41:15) \n",
      "[GCC 9.3.0]\n",
      "### NumPy version: 1.19.5\n",
      "### Scikit-learn version: 0.24.0\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "if name == 'enter your name' or email == 'enter your email':\n",
    "    assert False, 'Enter your name & email first!'\n",
    "else:\n",
    "    print('Assignment 3 -- name: {}, email: {}\\n'.format(name, email))\n",
    "    \n",
    "    # Load packages we need\n",
    "    import sys\n",
    "    import os\n",
    "    import time\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import sklearn\n",
    "\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "    # Let's check our software versions\n",
    "    print('### Python version: ' + __import__('sys').version)\n",
    "    print('### NumPy version: ' + np.__version__)\n",
    "    print('### Scikit-learn version: ' + sklearn.__version__)\n",
    "    print('------------')\n",
    "\n",
    "\n",
    "    # load our packages / code\n",
    "    sys.path.insert(1, '../common/')\n",
    "    import utils\n",
    "    import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters to control behavior of the pre-processing, ML, analysis, etc.\n",
    "seed = 42\n",
    "\n",
    "# deterministic seed for reproducibility\n",
    "##rng = np.random.default_rng(seed)  # best practice but not fully implemented in scikit-learn\n",
    "np.random.seed(seed)\n",
    "\n",
    "prop_vec = [14, 3, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading and Pre-processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this assignment we'll load the Bike Sharing dataset (hourly)\n",
    "### This dataset contains features of users bike sharing/rental on an hourly basis.\n",
    "### The task is to predict how many users are sharing/renting a bike."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17379 entries, 0 to 17378\n",
      "Data columns (total 15 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   season      16320 non-null  float64\n",
      " 1   year        16231 non-null  float64\n",
      " 2   month       16304 non-null  float64\n",
      " 3   hour        16254 non-null  float64\n",
      " 4   holiday     16277 non-null  float64\n",
      " 5   weekday     16282 non-null  float64\n",
      " 6   workingday  16297 non-null  float64\n",
      " 7   weathersit  16324 non-null  float64\n",
      " 8   temp        16242 non-null  float64\n",
      " 9   atemp       16271 non-null  float64\n",
      " 10  hum         16252 non-null  float64\n",
      " 11  windspeed   16281 non-null  float64\n",
      " 12  registered  16244 non-null  float64\n",
      " 13  nsqrtc      16263 non-null  float64\n",
      " 14  count       17379 non-null  int64  \n",
      "dtypes: float64(14), int64(1)\n",
      "memory usage: 2.0 MB\n"
     ]
    }
   ],
   "source": [
    "### Note: this dataset has missing values (artificially introduced), which you'll need to fill in before you can train a model\n",
    "df = pd.read_csv('../data/bikesharehour.csv.gz', compression='gzip', header=0, na_values='?')\n",
    "\n",
    "# Check that we loaded the data as expected\n",
    "df_expected_shape = (17379, 15)\n",
    "\n",
    "assert df.shape == df_expected_shape, 'Unexpected shape of df!'\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>registered</th>\n",
       "      <th>nsqrtc</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  year  month  hour  holiday  weekday  workingday  weathersit  temp  \\\n",
       "0     1.0   0.0    NaN   0.0      0.0      6.0         0.0         1.0   NaN   \n",
       "1     1.0   0.0    NaN   1.0      0.0      6.0         0.0         1.0   NaN   \n",
       "2     1.0   0.0    1.0   2.0      0.0      6.0         0.0         1.0   0.0   \n",
       "3     1.0   0.0    1.0   3.0      0.0      6.0         0.0         1.0   0.0   \n",
       "4     1.0   0.0    1.0   4.0      0.0      6.0         0.0         1.0   0.0   \n",
       "\n",
       "   atemp  hum  windspeed  registered  nsqrtc  count  \n",
       "0    0.0  0.0        0.0        13.0    -5.0     16  \n",
       "1    0.0  0.0        0.0        32.0    -8.0     40  \n",
       "2    0.0  0.0        0.0        27.0    -7.0     32  \n",
       "3    0.0  0.0        0.0        10.0    -5.0     13  \n",
       "4    0.0  0.0        0.0         1.0     0.0      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## what does the data look like?\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are some NaNs which we'll have to impute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab all the data as a numpy matrix\n",
    "all_xy = df.to_numpy()\n",
    "\n",
    "col_names = [c for c in df.columns]\n",
    "features = col_names[:-1]\n",
    "target = col_names[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: ['season', 'year', 'month', 'hour', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'registered', 'nsqrtc'] --- target: count\n"
     ]
    }
   ],
   "source": [
    "print('features: {} --- target: {}'.format(features, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1059, 1148, 1075, 1125, 1102, 1097, 1082, 1055, 1137, 1108, 1127,\n",
       "       1098, 1135, 1116,    0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many NaNs in each column?\n",
    "np.sum(np.isnan(all_xy), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe: no NaNs in the target/value column\n",
    "### About 1000+ NaNs in each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into x and y\n",
    "all_x_nan = all_xy[:,:-1]\n",
    "all_y = all_xy[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 1] (10 points) Let's impute the missing values! Use Scikit-learn's SimpleImputer to replace all NaNs in 'all_x_nan' with the *most frequent* value in each column. Use copy=True and store the results in 'all_x' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "###* put your code here (~2-3 lines) *###\n",
    "mf_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent', copy=True)\n",
    "\n",
    "all_x_mf = mf_imputer.fit_transform(all_x_nan)\n",
    "all_x = all_x_mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the shape is correct\n",
    "assert all_x.shape == (17379, 14)\n",
    "\n",
    "# check that there are no more NaNs\n",
    "assert np.sum(np.sum(np.isnan(all_x), axis=0)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll min-max normalize the features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(copy=True)\n",
    "scaler.fit(all_x) \n",
    "\n",
    "scaled_all_x = scaler.transform(all_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12166, 14), (12166,), (2607, 14), (2607,), (2606, 14), (2606,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the data into train, test, val\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(scaled_all_x, all_y, prop_vec, shuffle=True, seed=seed)\n",
    "\n",
    "# sanity check shapes\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape, val_x.shape, val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 2] (30 points) Let's train linear models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2a] (2 points) Train a Linear Regression model using the default hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "### Train a linear regression model on the training data (train_x, train_y)\n",
    "### Call the resulting trained model 'lrmodel'\n",
    "###* put your code here (~1 line) *###\n",
    "lrmodel = LinearRegression().fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2: 0.847, Val  R^2: 0.833\n",
      "Train MSE: 5033.098, Val MSE: 5532.778\n",
      "Train MAE: 39.039, Val MAE: 39.742\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def r2_mse_mae_eval(model, pref=''):\n",
    "    # R^2 the coefficient of determination\n",
    "    r2_train = model.score(train_x, train_y)\n",
    "    r2_val = model.score(val_x, val_y)\n",
    "\n",
    "    print('{}Train R^2: {:.3f}, Val  R^2: {:.3f}'.format(pref, r2_train, r2_val))\n",
    "\n",
    "    train_pred = model.predict(train_x)\n",
    "    val_pred = model.predict(val_x)\n",
    "\n",
    "    # measure the error (MSE) wrt true target\n",
    "    train_error = mean_squared_error(train_pred, train_y)\n",
    "    val_error = mean_squared_error(val_pred, val_y)\n",
    "\n",
    "    print('{}Train MSE: {:.3f}, Val MSE: {:.3f}'.format(pref, train_error, val_error))\n",
    "    \n",
    "    train_error = mean_absolute_error(train_pred, train_y)\n",
    "    val_error = mean_absolute_error(val_pred, val_y)\n",
    "\n",
    "    print('{}Train MAE: {:.3f}, Val MAE: {:.3f}'.format(pref, train_error, val_error))\n",
    "    \n",
    "r2_mse_mae_eval(lrmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2b] (3 points) How good is that model? (A few sentences is fine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "#\n",
    "# It's definitely a good model judging by the R^2 score. It's not perfect, of course.\n",
    "# Note: it's hard to say precisely how good without some kind of baseline... \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's setup some functions so we can tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some code to do a grid search and automatically train & evaluate the model with the best hyperparams.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def do_grid_search(model, param_grid, x, y):\n",
    "    gs = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error')\n",
    "    gs_res = gs.fit(x, y)\n",
    "    return  gs_res.best_params_\n",
    "\n",
    "\n",
    "def search_train_eval(model, param_grid, tr_x=train_x, tr_y=train_y, v_x=val_x, v_y=val_y):\n",
    "    \n",
    "    # since we do CV for the grid search, let's concatenate the train and val sets for it\n",
    "    search_x = np.r_[tr_x, v_x]\n",
    "    search_y = np.r_[tr_y, v_y]\n",
    "    \n",
    "    hyperparams = do_grid_search(model, param_grid, search_x, search_y)\n",
    "    \n",
    "    class_obj = type(model)\n",
    "    m = class_obj(**hyperparams).fit(tr_x, tr_y)\n",
    "    \n",
    "    cn = str(class_obj).split(\"'\")[1]\n",
    "    cn = cn.split('.')[-1]\n",
    "    print('{}({})'.format(cn, hyperparams))\n",
    "\n",
    "    r2_mse_mae_eval(m, pref='\\t')\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2c] (5 points) Do a grid search to tune hyperparameters and train an ElasticNet model. You can choose the values of hyperparameters your search over, but you must search over 'alpha' and 'l1_ratio'. Ensure that during the search the training of models converges in all cases (you should set max_iter or change your chosen values). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet({'alpha': 0.01, 'l1_ratio': 1.0})\n",
      "\tTrain R^2: 0.847, Val  R^2: 0.833\n",
      "\tTrain MSE: 5033.159, Val MSE: 5531.757\n",
      "\tTrain MAE: 39.032, Val MAE: 39.729\n"
     ]
    }
   ],
   "source": [
    "### Hint: you should define a parameter grid dictionary and call search_train_eval() to do the actual search\n",
    "###* put your code here (~3 lines) *###\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "param_grid = {'alpha':[0.01, 0.1, 1.0, 10.0], 'l1_ratio': [0.1, 0.5, 1.0]}\n",
    "enmodel = search_train_eval(ElasticNet(max_iter=1000), param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2d] (2 points) Do a grid search to tune hyperparameters and train a Ridge Regression model. You can choose the values of hyperparameters your search over, but you must search over 'alpha'. Ensure that during the search the training of models converges in all cases (you should set max_iter or change your chosen values). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge({'alpha': 0.1})\n",
      "\tTrain R^2: 0.847, Val  R^2: 0.833\n",
      "\tTrain MSE: 5033.101, Val MSE: 5532.577\n",
      "\tTrain MAE: 39.046, Val MAE: 39.749\n"
     ]
    }
   ],
   "source": [
    "###* put your code here (~3 lines) *###\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "param_grid = {'alpha':[0.01, 0.1, 1.0, 10.0]}\n",
    "rmodel = search_train_eval(Ridge(max_iter=1000), param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2e] (3 points) Print the parameter values (w and b) of the ElasticNet and Ridge Regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet model -- w: [ 26.043  14.238 -10.552  52.773 -7.933  6.554 -30.398 -30.660  0.000\n",
      " -0.000 -12.625  28.327  926.905 -14.186], b: 20.531\n",
      "Ridge model -- w: [ 26.431  14.271 -10.972  52.865 -8.324  6.631 -30.472 -30.828  1.476\n",
      " -16.757 -13.215  28.873  926.780 -14.613], b: 20.661\n"
     ]
    }
   ],
   "source": [
    "# Print the weights and bias for both models\n",
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "\n",
    "### Make sure you print the weights and bias for both models and that it is clear which is which.\n",
    "###* put your code here (~2 lines) *###\n",
    "print('ElasticNet model -- w: {}, b: {:.3f}'.format(enmodel.coef_, enmodel.intercept_))\n",
    "print('Ridge model -- w: {}, b: {:.3f}'.format(rmodel.coef_, rmodel.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2f] (2 points) How similar are the parameter values of the two models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.388,  0.033,  0.420,  0.092,  0.391,  0.077,  0.074,  0.168,\n",
       "         1.476,  16.757,  0.590,  0.546,  0.125,  0.427]),\n",
       " 'atemp')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "# Parameters are similar in some ways but different in other ways. The intercept is similar, \n",
    "# but the weights of some features are quite different (since the models perform similarly, it \n",
    "# suggests some features may be correlated to others)\n",
    "#\n",
    "#\n",
    "absv = np.abs(enmodel.coef_ - rmodel.coef_)\n",
    "absv, features[np.argmax(absv)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2g] (8 points) For each of the two models, display the three most important features alongside with their coefficients. Are these the same across both models?\n",
    "### What are the coefficients? Which feature is the most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ElasticNet] Feature 12 (registered) has weight: 926.905\n",
      "[Ridge] Feature 12 (registered) has weight: 926.780\n",
      "\n",
      "[ElasticNet] Feature 3 (hour) has weight: 52.773\n",
      "[Ridge] Feature 3 (hour) has weight: 52.865\n",
      "\n",
      "[ElasticNet] Feature 7 (weathersit) has weight: -30.660\n",
      "[Ridge] Feature 7 (weathersit) has weight: -30.828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###* put your code here *###\n",
    "feat_idx_elasticnet = np.flip(np.argsort(np.abs(enmodel.coef_)))\n",
    "feat_idx_ridge = np.flip(np.argsort(np.abs(rmodel.coef_)))\n",
    "\n",
    "for i in range(0, 3):\n",
    "    enidx = feat_idx_elasticnet[i]\n",
    "    print('[ElasticNet] Feature {} ({}) has weight: {:.3f}'.format(enidx, features[enidx], enmodel.coef_[enidx]))\n",
    "    ridx = feat_idx_ridge[i]\n",
    "    print('[Ridge] Feature {} ({}) has weight: {:.3f}'.format(ridx, features[ridx], rmodel.coef_[ridx]))\n",
    "    print()\n",
    "\n",
    "\n",
    "###* put your answer as comment here *###\n",
    "# \n",
    "# ElasticNet: registered (923.807), hour (53.326), weathersit (-31.456)\n",
    "# Ridge: registered (923.725), hour (53.412), weathersit (-31.623)\n",
    "#\n",
    "# The two models are very similar\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2h] (5 points) Take a look at the code of search_train_eval() and do_grid_search(). Answer the following questions: \n",
    "### 1. Why is the scoring function for the grid search 'neg_mean_squared_error' (as opposed to 'mean_squared_error')? \n",
    "### 2. Why is it okay to do the search over search_x and search_y which are the concatenation of the train and 'validation sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hint: take a look at the documentation of scikit-learn for GridSearchCV and related classes.\n",
    "###* put your answer as comment here *###\n",
    "#\n",
    "# 1. GridSearchCV (like other hyperparam tuning of scikit-learn) is set up to pick the hyperparam combination\n",
    "# that maximizes a score function. So if we want to minimize MSE, we can equivalently maximize -MSE.\n",
    "#\n",
    "# 2.GridSearchCV is doing cross-validation and averaging results. So it's okay if we combine train and val because\n",
    "# it will not overfit the search on the validation set (due to cross-val & averaging). \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 3] (30 points) Let's train polynomial regression models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3a] (5 points) Use PolynomialFeatures to create a version of the data with all features of degree 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "### Use PolynomialFeatures to create a version of the data with all features of degree 2. Make sure to allow interactions (interaction_only=False) and set include_bias=False.\n",
    "### Store the result in 'all_x_polyf'. Ensure that you make a copy of the original data and you use the scaled features ('scaled_all_x')!\n",
    "###* put your code here (~2 lines) *###\n",
    "polyf = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "all_x_polyf = polyf.fit_transform(scaled_all_x.copy())\n",
    "\n",
    "assert all_x_polyf.shape == (17379, 119)\n",
    "\n",
    "# split the data into train, test, val\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(all_x_polyf, all_y, prop_vec, shuffle=True, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train a LinearRegression model and a Ridge model on our polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2: 0.927, Val  R^2: 0.923\n",
      "Train MSE: 2412.444, Val MSE: 2553.487\n",
      "Train MAE: 24.959, Val MAE: 24.840\n",
      "\n",
      "Train R^2: 0.926, Val  R^2: 0.921\n",
      "Train MSE: 2444.731, Val MSE: 2634.107\n",
      "Train MAE: 24.644, Val MAE: 24.661\n"
     ]
    }
   ],
   "source": [
    "# Train a linear regression model\n",
    "pf_lrmodel = LinearRegression().fit(train_x, train_y)\n",
    "r2_mse_mae_eval(pf_lrmodel)\n",
    "\n",
    "print()\n",
    "\n",
    "# Train a Ridge regression model\n",
    "pf_ridgemodel = Ridge(alpha=0.5).fit(train_x, train_y)\n",
    "r2_mse_mae_eval(pf_ridgemodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3b] (5 points) What is the difference between LinearRegression and Ridge? (A sentence or two is fine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "# Ridge will do L2-regularization whereas LinearRegression will do not any regularization (OLS).\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3c] (5 points) Look at (e.g., print) the parameters of both the LinearRegression model ('pf_lrmodel') and the Ridge model ('pf_ridgemodel'). What do you notice? Explain what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression model (poly. f.) -- w: [ 5.684  153343876892390.719  41.607  123.876 -54293547848441.305  26.913\n",
      "  1878227560857.761  34.037 -14021413734269.637 -4597994788482.195\n",
      "  595998891306.100 -23.689  721.223 -1788.284 -16.017  13.763  25.116\n",
      "  43.184  1.287 -5.791  0.372 -11.271 -9480688549146.117\n",
      " -3966266098589.549  1.123  11.148 -83.558 -18.907 -153343876892383.438\n",
      " -11.635  40.681 -6.398  1.560  5.211 -10.750  6301087549983.425\n",
      " -1005791667980.723 -0.895  23.811 -108.793 -16.492 -40.672 -5.789 -4.735\n",
      "  5.906 -1.596  6.668  3219927304071.929 -559213520135.313 -0.695 -5.703\n",
      "  1.354  1.609 -110.365 -0.508  9.141  28.551 -46.773  3742573190253.013\n",
      " -574688232509.478 -17.676  47.791 -230.324 -10.719  54293547848427.492\n",
      " -7.742 -18.162 -5.323 -119809244824.540 -211884697337.347\n",
      " -99807676006.857  42.522 -6.065  20.273 -25.729  3.786  0.656\n",
      "  6710637521339.792  967483716330.801 -3.277 -30.202 -23.461  9.766\n",
      " -1878227560865.586 -10.750 -49666834501.250  2705499065414.198 -7.961\n",
      "  8.312 -48.875  0.043 -30.134  0.000  0.000  3.863 -35.221  104.133\n",
      " -0.381  2357540194037.926  0.000  0.000  933231646961.164\n",
      "  1162970161821.980  811223925968.616  3088088698793.254  0.000\n",
      "  1730279442767.561  8296277303.516  1260091281890.876 -595998891341.000\n",
      "  8.770  96.789  66.302 -21.646 -98.812  67.238  512.676  75.444  1769.781], b: 430.545\n",
      "\n",
      "Ridge model (poly. f.) -- w: [ 16.347  7.337  66.439  176.621 -6.577  24.568 -2.569  31.044 -5.120\n",
      " -6.905  2.513  2.124  717.231 -1469.418 -22.315  13.513  23.884  35.374\n",
      "  5.887 -3.275  0.453 -8.459 -3.414 -4.604 -7.073  14.000 -67.229 -26.480\n",
      "  7.337 -13.644  38.798 -5.140  2.087  6.122 -10.807 -5.120  0.000 -2.165\n",
      "  16.034 -102.178 -25.542 -54.080 -1.729 -4.261  3.096 -0.667  2.656\n",
      " -2.793 -3.767  2.529 -8.909  1.434 -17.343 -136.208  0.321  8.279  26.105\n",
      " -48.626 -3.562 -4.203 -14.097  40.617 -169.384 -56.432 -6.577 -9.999\n",
      " -4.715  7.348  0.000  0.000  0.000  25.655 -12.335  17.473 -17.424  3.010\n",
      "  2.255 -5.120 -5.755 -8.206 -28.720 -11.958 -3.935 -2.569 -7.197  0.000\n",
      " -6.905 -1.123  6.509 -87.675 -2.441 -30.328  0.000  0.000  6.964 -44.029\n",
      "  97.383  2.161 -5.120  0.000  0.000 -0.904 -1.110 -1.237 -6.905  0.000\n",
      " -2.031 -0.600 -4.316  2.513  5.425  40.523 -9.988 -21.331 -73.229  29.575\n",
      "  485.084  92.357  1513.202], b: 328.732\n"
     ]
    }
   ],
   "source": [
    "###* put your code here *###\n",
    "print('LinearRegression model (poly. f.) -- w: {}, b: {:.3f}'.format(pf_lrmodel.coef_, pf_lrmodel.intercept_))\n",
    "print()\n",
    "print('Ridge model (poly. f.) -- w: {}, b: {:.3f}'.format(pf_ridgemodel.coef_, pf_ridgemodel.intercept_))\n",
    "\n",
    "###  What do you notice? Explain what is going on.\n",
    "###* put your answer as comment here *###\n",
    "#\n",
    "# Values of the parameters/coefficients for the LinearRegression models are all over the place (some have very large magnitude both neg. and pos.). \n",
    "# This is because the model has no regularizing constraints! In contrast, coefficients of the Ridge model seem more reasonable.\n",
    "# Note: the regularization constant (alpha) for Ridge is not particularly large which means the regularization effect is small\n",
    "# yet this has a significant effect on the coefficients!\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3d] (5 points) Focus on the Ridge model. What are the three most important features? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 118 (nsqrtc^2) has weight: 1513.202\n",
      "Feature 13 (nsqrtc) has weight: -1469.418\n",
      "Feature 12 (registered) has weight: 717.231\n"
     ]
    }
   ],
   "source": [
    "### Print the three most important features alongside with their weights.\n",
    "### Hint: you can use get_feature_names() method of PolynomialFeatures to relate polynomial features to the original features.\n",
    "###* put your code here *###\n",
    "\n",
    "poly_features = polyf.get_feature_names(features)\n",
    "feat_idx = np.flip(np.argsort(np.abs(pf_ridgemodel.coef_)))\n",
    "\n",
    "for i in range(0, 3):\n",
    "    print('Feature {} ({}) has weight: {:.3f}'.format(feat_idx[i], poly_features[feat_idx[i]], pf_ridgemodel.coef_[feat_idx[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3e] (5 points) Let's use only these three most important features. Extract the three features from the polynomial features to create a new feature matrix with three columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract the three features from 'all_x_polyf' and store the results in 'all_x_3most'\n",
    "###* put your code here *###\n",
    "\n",
    "all_x_3most = all_x_polyf[:, [feat_idx[0], feat_idx[1], feat_idx[2]]]\n",
    "\n",
    "assert all_x_3most.shape == (17379, 3)\n",
    "\n",
    "# split the data into train, test, val\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(all_x_3most, all_y, prop_vec, shuffle=True, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3f] (2 points) Now train a LinearRegression model (default hyperparams) on the training data from 'all_x_3most'. What do you observe about the performance of this model? What is your conclusion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2: 0.912, Val  R^2: 0.904\n",
      "Train MSE: 2903.069, Val MSE: 3181.395\n",
      "Train MAE: 23.434, Val MAE: 23.624\n"
     ]
    }
   ],
   "source": [
    "###* put your code here *###\n",
    "threemost_model = LinearRegression().fit(train_x, train_y)\n",
    "r2_mse_mae_eval(threemost_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3g] (3 points) How good is that model? What do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "# We observe that the model is not quite as good. However, it's performance is not that far off. \n",
    "# Considering that this model only relies on three features and thus is less likely to overfit, it is a reasonable alternative. \n",
    "# because of the three features is a polynomial feature, we also conclude that the relationship in the data \n",
    "# between features and target is somewhat non-linear.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 4] (30 points) Trees, More Trees, lots of Trees!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to reset the data to the original form (before polynomial features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do some cleanup\n",
    "del train_x, train_y, test_x, test_y, val_x, val_y\n",
    "\n",
    "# split the data into train, test, val\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(scaled_all_x, all_y, prop_vec, shuffle=True, seed=seed)\n",
    "\n",
    "# sanity check shapes\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape, val_x.shape, val_y.shape\n",
    "assert train_x.shape == (12166, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train a decision tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "dtmodel = DecisionTreeRegressor(random_state=seed).fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment the code in the cell below if you have some time to wait around and want to visualize the tree, otherwise skip it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This will take a long time (10-20 minutes); skip if you are in a hurry\n",
    "# let's plot what the tree looks like\n",
    "#plt.figure(figsize=(16,12))\n",
    "#\n",
    "#plot_tree(dtmodel, feature_names=features, filled=True, label='all', rounded=True)\n",
    "#\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4a] (10 points) Answer some questions about the structure of our tree (dtmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Can the tree be visualized easily?\n",
    "#### 2. How deep is the tree?\n",
    "#### 3. How many nodes it contain?\n",
    "#### 4. How many total splits are there?\n",
    "#### 5. What is the impurity of the last 2 nodes?\n",
    "#### Hint: lookup the scikit-learn documentation to know how to manipulate the 'tree_' attribute of Decision Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18157, 35, 9078.0, array([ 0.000]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "# 1. Not easily no, the tree is huge!\n",
    "# 2. depth = 35\n",
    "# 3. 18157\n",
    "# 4. 9078. We can reason as follows: a binary tree of n nodes has n-1 edges, so it must have (n-1)/2 splits (2 edges per split).\n",
    "# 5. 0\n",
    "#\n",
    "nodes = dtmodel.tree_.node_count\n",
    "nodes, dtmodel.tree_.max_depth, (nodes-1)/2, dtmodel.tree_.impurity[dtmodel.tree_.node_count-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's evaluate the decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2: 1.000, Val  R^2: 0.968\n",
      "Train MSE: 0.000, Val MSE: 1076.791\n",
      "Train MAE: 0.000, Val MAE: 10.691\n"
     ]
    }
   ],
   "source": [
    "r2_mse_mae_eval(dtmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4b] (5 points) Is it a good model? Is it overfitted? Is it better than the models trained in Tasks 2 and 3? (A few sentences suffice.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "# It's a much better model in terms of R^2, MSE, MAE on the validation data compared to the other models from Tasks 2 & 3.\n",
    "# It's also a very overfitted model as we can see by the fact that the training MSE/MAE are both 0.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4c] (5 points) Train another decision tree but this time regularize it. Can you obtain a model with similar performance to 'dtmodel' but not (or at least less) overfitted? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2: 0.990, Val  R^2: 0.966\n",
      "Train MSE: 318.945, Val MSE: 1120.187\n",
      "Train MAE: 6.102, Val MAE: 11.059\n"
     ]
    }
   ],
   "source": [
    "### Call your new model 'dtregmodel'\n",
    "###* put your code here *###\n",
    "\n",
    "dtregmodel = DecisionTreeRegressor(random_state=seed, max_depth=20, min_samples_split=12).fit(train_x, train_y)\n",
    "\n",
    "\n",
    "r2_mse_mae_eval(dtregmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4d] (5 points) Now let's train a random forest and see if we can train an even better model. Use search_train_eval() to do a grid search over hyperparameters. You are free to pick whatever hyperparameters & values you want, but you should try to avoid badly overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor({'max_depth': 50, 'min_samples_split': 5})\n",
      "\tTrain R^2: 0.996, Val  R^2: 0.983\n",
      "\tTrain MSE: 141.907, Val MSE: 578.337\n",
      "\tTrain MAE: 3.639, Val MAE: 7.728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "### Name your random forest model 'rfmodel'\n",
    "### Make sure to set random_state=seed for reproducibility!\n",
    "###* put your code here *###\n",
    "\n",
    "param_grid = {'max_depth':[10, 20, 50, 100], 'min_samples_split': [5, 10, 50]}\n",
    "rfmodel = search_train_eval(RandomForestRegressor(random_state=seed), param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4e] (5 points) Is your model better than the decision tree you trained for Task 4c? Justify your answer. What do you conclude about ensembles/random forests?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "# It's a better model with lower validation error, but it does still overfit significantly.\n",
    "# This result suggests that ensemble techniques (specifically random forests) \n",
    "# can produce overall better models than a single model (of the type used for the weak learners) .\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [CIS6930 Additional Task -- Task 5] (25 points): Stacking Meta Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this task we'll use stacking to create a meta model or blender model to predict the target using predictions from 6 other models from Tasks 1 - 4 as features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5a] (10 points) Fill in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "# these are the models we'll use from previous tasks\n",
    "regressors = [('lr', lrmodel), ('elasticnet', enmodel), ('ridge', rmodel), \n",
    "             ('dt', dtmodel), ('dtreg', dtregmodel), ('rf', rfmodel)]\n",
    "\n",
    "def regressors_preds(x):\n",
    "    num_regs = len(regressors)\n",
    "    \n",
    "    ### Create an array to contain the predictions from the regressors over all examples in 'x'\n",
    "    ### Each regressor will correspond to one feature (i.e., one column)\n",
    "    ### The numpy array you return should have shape (x.shape[0], num_regs)\n",
    "    ###* put your code here (~4-6 lines) *###\n",
    "    res = np.zeros((x.shape[0],num_regs))\n",
    "    \n",
    "    for i in range(0, num_regs):\n",
    "        name, regr = regressors[i]\n",
    "        \n",
    "        pred_reg = regr.predict(x)\n",
    "        res[:,i] = pred_reg\n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "def stacking_train_eval(model_name, model, standardize=False):\n",
    "    ### Create a new training dataset meta_train_x and meta_train_y\n",
    "    ### For this use the validation data (val_x, val_y) alongside with regressors_preds()\n",
    "    ###* put your code here (~2 lines) *###\n",
    "    meta_train_x = regressors_preds(val_x)\n",
    "    meta_train_y = val_y\n",
    "    \n",
    "    assert meta_train_x.shape == (2606, 6) and meta_train_x.shape[0] == meta_train_y.shape[0]\n",
    "\n",
    "    ### Create our new test dataset meta_test_x and meta_test_y\n",
    "    ### For this use the test data (test_x, test_y) alongside with regressors_preds()\n",
    "    ###* put your code here (~2 lines) *###\n",
    "    meta_test_x = regressors_preds(test_x)\n",
    "    meta_test_y = test_y\n",
    "    \n",
    "    assert meta_test_x.shape == (2607, 6) and meta_test_x.shape[0] == meta_test_y.shape[0]\n",
    "    \n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        meta_train_x = scaler.fit_transform(meta_train_x)\n",
    "        meta_test_x = scaler.transform(meta_test_x)\n",
    "\n",
    "    # train the model\n",
    "    model.fit(meta_train_x, meta_train_y)\n",
    "\n",
    "    # make predictions & eval\n",
    "    train_pred = model.predict(meta_train_x)\n",
    "    test_pred = model.predict(meta_test_x)\n",
    "\n",
    "    train_error = mean_squared_error(train_pred, meta_train_y)\n",
    "    val_error = mean_squared_error(test_pred, test_y)\n",
    "\n",
    "    print('Stacking (Meta model: {})'.format(model_name))\n",
    "    print('\\tTrain MSE: {:.3f}, Test MSE: {:.3f}'.format(train_error, val_error))\n",
    "\n",
    "    train_error = mean_absolute_error(train_pred, meta_train_y)\n",
    "    val_error = mean_absolute_error(test_pred, test_y)\n",
    "\n",
    "    print('\\tTrain MAE: {:.3f}, Test MAE: {:.3f}'.format(train_error, val_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5b] (3 points) Train a SVM regression model with a linear kernel and C=100. Use stacking_train_eval(). You can set standardize=True to zscore normalize features.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking (Meta model: SVM(linear kernel, C=100))\n",
      "\tTrain MSE: 559.621, Test MSE: 722.716\n",
      "\tTrain MAE: 7.665, Test MAE: 7.823\n"
     ]
    }
   ],
   "source": [
    "### Train a SVM regressor with a linear kernel and C=100\n",
    "### Note: the training will take a few minutes\n",
    "###* put your code here (~2 lines) *###\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svm = SVR(kernel='linear', C=100)\n",
    "stacking_train_eval('SVM(linear kernel, C=100)', svm, standardize=True)\n",
    "\n",
    "# note: not rescaling the data makes it take longer but the final model is similar\n",
    "#stacking_train_eval('SVM(linear kernel, C=100)', svm, standardize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5c] (2 points) How good is this model? (A few sentences suffice.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "# The model is not as good as the random forests model (in terms of prediction on test data)\n",
    "# but it does not overfit as much!\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5d] (5 points) Now train a SVM regression model with any other kernel (i.e., not linear) and combination of hyperparameters of your choice. Can you train a better stacking model than for Task 5b?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking (Meta model: SVM(rbf, C=1.0))\n",
      "\tTrain MSE: 6431.484, Test MSE: 6417.310\n",
      "\tTrain MAE: 28.326, Test MAE: 28.700\n",
      "Stacking (Meta model: SVM(rbf, C=10))\n",
      "\tTrain MSE: 1126.260, Test MSE: 1308.316\n",
      "\tTrain MAE: 10.717, Test MAE: 11.367\n",
      "Stacking (Meta model: SVM(rbf, C=100))\n",
      "\tTrain MSE: 567.578, Test MSE: 813.794\n",
      "\tTrain MAE: 7.577, Test MAE: 8.297\n",
      "Stacking (Meta model: SVM(rbf, C=1000))\n",
      "\tTrain MSE: 458.209, Test MSE: 804.324\n",
      "\tTrain MAE: 6.970, Test MAE: 7.904\n",
      "Stacking (Meta model: SVM(rbf, C=10000))\n",
      "\tTrain MSE: 427.715, Test MSE: 853.571\n",
      "\tTrain MAE: 6.485, Test MAE: 8.083\n",
      "Stacking (Meta model: SVM(poly, C=1.0))\n",
      "\tTrain MSE: 18487.651, Test MSE: 17543.485\n",
      "\tTrain MAE: 101.286, Test MAE: 98.914\n",
      "Stacking (Meta model: SVM(poly, C=10))\n",
      "\tTrain MSE: 19486.855, Test MSE: 19704.250\n",
      "\tTrain MAE: 98.417, Test MAE: 97.711\n",
      "Stacking (Meta model: SVM(poly, C=100))\n",
      "\tTrain MSE: 19282.481, Test MSE: 20731.396\n",
      "\tTrain MAE: 97.447, Test MAE: 97.956\n",
      "Stacking (Meta model: SVM(poly, C=1000))\n",
      "\tTrain MSE: 18753.085, Test MSE: 20501.573\n",
      "\tTrain MAE: 96.831, Test MAE: 98.145\n",
      "Stacking (Meta model: SVM(poly, C=10000))\n",
      "\tTrain MSE: 18623.099, Test MSE: 20707.377\n",
      "\tTrain MAE: 96.552, Test MAE: 98.203\n",
      "Stacking (Meta model: SVM(sigmoid, C=1.0))\n",
      "\tTrain MSE: 30244.669, Test MSE: 29874.324\n",
      "\tTrain MAE: 96.985, Test MAE: 96.586\n",
      "Stacking (Meta model: SVM(sigmoid, C=10))\n",
      "\tTrain MSE: 1315104.543, Test MSE: 1276816.323\n",
      "\tTrain MAE: 740.050, Test MAE: 730.289\n",
      "Stacking (Meta model: SVM(sigmoid, C=100))\n",
      "\tTrain MSE: 119226504.610, Test MSE: 115521917.289\n",
      "\tTrain MAE: 7197.459, Test MAE: 7090.300\n",
      "Stacking (Meta model: SVM(sigmoid, C=1000))\n",
      "\tTrain MSE: 11813263451.919, Test MSE: 11443764799.818\n",
      "\tTrain MAE: 71771.725, Test MAE: 70692.214\n",
      "Stacking (Meta model: SVM(sigmoid, C=10000))\n",
      "\tTrain MSE: 1179585479913.682, Test MSE: 1142633476889.081\n",
      "\tTrain MAE: 717514.125, Test MAE: 706701.596\n"
     ]
    }
   ],
   "source": [
    "### Train a SVM regressor with any non-linear kernel and hyperparameters you want\n",
    "###* put your code here (~2 lines) *###\n",
    "for kernel in ['rbf', 'poly', 'sigmoid']:\n",
    "    for C in [1.0, 10, 100, 1000, 10000]:\n",
    "        svm = SVR(kernel=kernel, C=C)\n",
    "\n",
    "        stacking_train_eval('SVM({}, C={})'.format(kernel, C), svm, standardize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5e] (5 points) What do you conclude? Provide a plausible explanation why non-linear kernels do not seem to improve the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "# We conclude that stacking can work. In this case though stacking does not provide a significantly better model\n",
    "# in terms of validation/test data compared to the RF model we trained in Task 4.\n",
    "#\n",
    "# A plausible explanation is that since the features are predictions from regressors, \n",
    "# the best way to combine them is through a linear combination. This would explain why kernels such RBF have poor performance.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
